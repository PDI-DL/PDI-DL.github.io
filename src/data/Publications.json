[
    {
        "title": "AquaFeat+: an Underwater Vision Learning-based Enhancement Method for Object Detection, Classification, and Tracking",
        "conference":"IEEE International Conference on Advanced Robotics (ICAR)",
        "authors": "Emanuel C. Silva; Tatiana T. Schein; José D. G. Ramos; Eduardo L. Silva; Stephanie L. Brião; Felipe G. Oliveira; Paulo L. J. Drews-Jr",
        "year": 2025,
        "paper": ["https://ieeexplore.ieee.org/abstract/document/11338647?casa_token=NsgtsUtRhwAAAAAA:OFqdAYKBesOnf66-qGhmSVOvPAgn8iVi1ouz3MBGFSJw9pZRXxagGtvR-je3bN3vM1SMZctD3m5I"],
        "abstract": "Underwater video analysis is particularly challenging due to factors such as low lighting, color distortion, and turbidity, which compromise visual data quality and directly impact the performance of perception modules in robotic applications. This work proposes AquaFeat+, a plug-and-play pipeline designed to enhance features specifically for automated vision tasks, rather than for human perceptual quality. The architecture includes modules for color correction, hierarchical feature enhancement, and an adaptive residual output, which are trained end-to-end and guided directly by the loss function of the final application. Trained and evaluated in the FishTrack23 dataset, AquaFeat+ achieves significant improvements in object detection, classification, and tracking metrics, validating its effectiveness for enhancing perception tasks in underwater robotic applications."
    },
        {
        "title": "AquaFeat: A Features-Based Image Enhancement Model for Underwater Object Detection",
        "conference": "38th Conference on Graphics, Patterns and Images (SIBGRAPI)",
        "authors": "Emanuel C. Silva; Tatiana T. Schein; Stephanie L. Brião; Guilherme L. M. Costa; Felipe G. Oliveira; Gustavo P. Almeida; Eduardo L. Silva; Sam S. Devincenzi; Karina S. Machado; Paulo L. J. Drews-Jr",
        "year": 2025,
        "paper": ["https://ieeexplore.ieee.org/abstract/document/11223382?casa_token=TafOH_qgggsAAAAA:y_MHXJe17F_QOhwGhnIV_3_iNTHSsea-wWcE-59TzlPNdpM8YzI6xk_BIZTaE8-dKC52NdQ9AYzk", "https://arxiv.org/abs/2508.12343"],
        "abstract": "The severe image degradation in underwater environments impairs object detection models, as traditional image enhancement methods are often not optimized for such downstream tasks. To address this, we propose AquaFeat, a novel, plug-and-play module that performs task-driven feature enhancement. Our approach integrates a multi-scale feature enhancement network trained end-to-end with the detector's loss function, ensuring the enhancement process is explicitly guided to refine features most relevant to the detection task. When integrated with YOLOv8m on challenging underwater datasets, AquaFeat achieves state-of-the-art Precision (0.877) and Recall (0.624), along with competitive m A P scores (mAP0.5 of 0.677 and mAP0.5:0.95 of 0.421). By delivering these accuracy gains while maintaining a practical processing speed of 46.5 FPS, our model provides an effective and computationally efficient solution for real-world applications, such as marine ecosystem monitoring and infrastructure inspection."
    },
    {
        "title": "Sonar Image Datasets: A Comprehensive Survey of Resources, Challenges, and Applications",
        "conference": "38th SIBGRAPI Conference on Graphics, Patterns and Images",
        "authors": "Larissa S. Gomes; Gustavo P. Almeida; Bryan U. Moreira; Marco Quiroz; Breno Xavier; Lucas Soares; Stephanie L. Brião; Felipe G. Oliveira; Paulo L. J. Drews-Jr",
        "year": 2025,
        "paper": ["https://arxiv.org/abs/2510.03353"],
        "abstract": "Research and usage of sonar images are relevant for advancing underwater exploration, autonomous navigation, and ecosystem monitoring. However, the progress depends on data availability. The scarcity of publicly available, well-annotated sonar image datasets creates a significant bottleneck for the development of robust machine learning models. This article presents a comprehensive and concise review of the current landscape of sonar image datasets, seeking not only to catalog existing resources but also to contextualize them, identify gaps, and provide a clear roadmap, serving as a base guide for researchers of any kind who wish to start or advance in the field of underwater acoustic data analysis. We mapped publicly accessible datasets across various sonar modalities, including Side Scan Sonar (SSS), Forward-Looking Sonar (FLS), Synthetic Aperture Sonar (SAS), Multibeam Echo Sounder (MBES), and Dual-Frequency Identification Sonar (DIDSON). An analysis was conducted on applications such as classification, detection, segmentation, and 3D reconstruction. This work focuses on state-of-the-art advancements, incorporating newly released datasets. The findings are synthesized into a master table and a chrono-logical timeline, offering a clear and accessible comparison of characteristics, sizes, and annotation details datasets."
    },
    {
        "title": "DualMatch: A Dual EMA Teacher for Underwater Semi-Supervised Pipeline Segmentation",
        "conference": "38th SIBGRAPI Conference on Graphics, Patterns and Images",
        "authors": "Eduardo L. Silva; Tatiana T. Schein; Stephanie L. Brião; Gabriel L. Anastacio; Felipe G. Oliveira and Paulo L. J. Drews-Jr",
        "year": 2025,
        "paper": ["https://ieeexplore.ieee.org/document/11223277/"],
        "abstract": "Underwater pipeline semantic segmentation remains a challenging task due to visual distortions, limited labeled data, and complex class boundaries. In this work, DualMatch is proposed, a semi-supervised segmentation framework based on a Dual Teacher architecture with Exponential Moving Average (EMA) models. Unlike prior approaches that rely on a single teacher, our method leverages two complementary teacher networks to reduce noise and enhance consistency in generating pseudo-labels. The powerful DINOv2-Small is adopted as the encoder, and our model is evaluated on a challenging underwater dataset. To support this research, a dataset comprising images from Remotely Operated Vehicle (ROV) and Autonomous Underwater Vehicle (AUV) inspections is proposed, with semantic labels manually annotated for five object classes. Experimental results show that DualMatch achieves the best performance in four out of five semantic classes, and notably reaches an IoU of 84.69% in the Pipeline class, outperforming all compared methods, including UniMatchv2 and Allspark. Overall, the proposed approach sets a new standard in underwater semantic segmentation by delivering consistent and high-accuracy predictions across complex classes, most notably for pipeline segmentation."
    },
    {
        "title": "Autoencoder Satellite Image Matching for UAV Geolocation in Long-Range High-Altitude Missions",
        "conference": "37th SIBGRAPI Conference on Graphics, Patterns and Images",
        "authors": "Lucas B. V. Cordova; Stephanie L. Brião; Felipe G. Oliveira; Rodrigo S. Guerra; Paulo L. J. Drews-Jr",
        "year": 2024,
        "paper": ["https://ieeexplore.ieee.org/document/10716323"],
        "abstract": "Vision-based geolocation is a promising way to overcome the vulnerabilities of Global Navigation Satellite System (GNSS) methods, which are subject to signal degradation, intentional interference, and environmental obstacles. This paper presents a novel approach to Unmanned Aerial Vehicle (UAV) geolocation in long-range and high-altitude missions using satellite imagery. Our method is based on the matching of encoded vector representations in embedded space, demonstrating robust performance to changes in vegetation and landscape. The neural network is used to encode satellite images of a reference map into embedding representations. Image matching is performed in this embedded space using cross-correlation. We evaluated the accuracy and processing time of the proposed model by querying images along a 200 km northbound path at high altitude, covering an area larger than twenty thousand square kilometers. We also evaluated the network's generalization capability on an unknown map. Reference and query images are sourced from satellite images captured at different acquisition times to evaluate robustness due to appearance variations. The results demonstrate that the method can achieve up to 96.83% accuracy on a known map, while experiments on an unknown map averaged 90% accuracy. The processing time to match encoded images is 0.05 ms. These findings suggest the feasibility of integrating the method into more complex vision-based geolocation systems."
    }
]
